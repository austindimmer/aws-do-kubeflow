{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2953f3d8-8984-4295-ac0c-1da45f2cf56e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notebook 1 - Experiment Phase\n",
    "\n",
    "#### This notebook shall be used as a first step to experiment running distributed training using PyTorch Training Operators on Kuberntes. \n",
    "\n",
    "#### This notebook is designed to create the PyTorchJob custom resource manifest using Kubeflow training and Kubernetes python clients. The PyTorch Training Operators makes it easy to run distributed or non-distributed PyTorch jobs on Kubernetes. However, please feel free to log in to cloud9 or other clients  which connect to your Kubernetes cluster to run kubectl commands "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e281a1-c334-475f-875f-166f0faa6a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please run the below commands to install necessary libraries \n",
    "\n",
    "#!pip install kfp==1.8.4\n",
    "#!pip install kubeflow-training\n",
    "#!pip install kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "015c2418-539e-457e-a802-e54fc4f80437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Kubernetes client and kubeflow training operator pythion libraries. We will use this to create PyTorchJob manifest yaml file \n",
    "\n",
    "from kubernetes.client import V1PodTemplateSpec\n",
    "from kubernetes.client import V1ObjectMeta\n",
    "from kubernetes.client import V1PodSpec\n",
    "from kubernetes.client import V1Container\n",
    "from kubernetes.client import V1ResourceRequirements\n",
    "from kubernetes.client import V1VolumeMount\n",
    "from kubernetes.client import V1Volume\n",
    "from kubernetes.client import V1PersistentVolumeClaimVolumeSource\n",
    "\n",
    "from kubeflow.training import constants\n",
    "from kubeflow.training.utils import utils\n",
    "from kubeflow.training import V1ReplicaSpec\n",
    "from kubeflow.training import V1PyTorchJob\n",
    "from kubeflow.training import V1PyTorchJobSpec\n",
    "from kubeflow.training import PyTorchJobClient\n",
    "from kubeflow.training import V1RunPolicy\n",
    "\n",
    "import kfp\n",
    "from kfp import components\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "from pytorch_dist_utility import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ccfc107-9698-4cf8-b787-a1c5b8bba717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize global variables \n",
    "\n",
    "user_namespace = utils.get_default_target_namespace()\n",
    "\n",
    "pytorch_distributed_jobname=f'pytorch-cnn-dist-job-{time.strftime(\"%Y-%m-%d-%H-%M-%S-%j\", time.gmtime())}'\n",
    "\n",
    "efs_mount_point='efs-sc-claim'\n",
    "\n",
    "aws_dlc_pytorch_gpu_image='763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:1.12.0-gpu-py38-cu116-ubuntu20.04-e3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e934ac70-42ca-4fa7-9bf5-566ddbb16c02",
   "metadata": {},
   "source": [
    "## Create PyTorch Job CRD Yaml File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51b18e4b-f5e0-4c1d-b1f5-aafaf6d1ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Volume specification for PyTorchJob to be claimed by master and worker pods \n",
    "\n",
    "persistent_vol_claim = V1PersistentVolumeClaimVolumeSource(\n",
    "    claim_name=efs_mount_point\n",
    ")\n",
    "\n",
    "efs_volume = V1Volume(\n",
    "    name=efs_mount_point,\n",
    "    persistent_volume_claim=persistent_vol_claim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5811ccb7-d96c-4dc8-8724-b3b4f80b67aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create container specification for PyTorchJob master and worker pods \n",
    "\n",
    "# Mount volume to container pods\n",
    "efsvolumemount = V1VolumeMount(\n",
    "    mount_path=\"/\"+efs_mount_point,\n",
    "    name=efs_mount_point\n",
    ")\n",
    "\n",
    "# Resource configuration for master and worker containers\n",
    "resource_reqs = V1ResourceRequirements(\n",
    "    limits={'nvidia.com/gpu':'1'}\n",
    ")\n",
    "\n",
    "# Create master and worker container spec \n",
    "container = V1Container(\n",
    "    name=\"pytorch\",\n",
    "    image=aws_dlc_pytorch_gpu_image,\n",
    "    args=[\"python\",\"./\"+efs_mount_point+\"/cifar10-distributed-gpu-final.py\",\"--epochs\",\"3\",\"--seed\",\"7\",\"--log-interval\",\"60\",\"--efs-mount-path\",efs_mount_point,\"--efs-dir-path\",\"cifar10-dataset\"],    \n",
    "    volume_mounts=[efsvolumemount],\n",
    "    resources=resource_reqs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7613e3c-c8d2-4c7b-ab76-b3bd9837e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create master specification \n",
    "master = V1ReplicaSpec(\n",
    "    replicas=1,\n",
    "    restart_policy=\"OnFailure\",\n",
    "    template=V1PodTemplateSpec(\n",
    "        metadata=V1ObjectMeta(\n",
    "            annotations={'sidecar.istio.io/inject': 'false'}\n",
    "        ),\n",
    "        spec=V1PodSpec(\n",
    "            containers=[container],\n",
    "            volumes=[efs_volume]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create worker specification \n",
    "worker = V1ReplicaSpec(\n",
    "    replicas=2, # How many gpus or cpus shall be needed to distribute the training across\n",
    "    restart_policy=\"OnFailure\",\n",
    "    template=V1PodTemplateSpec(\n",
    "        metadata=V1ObjectMeta(\n",
    "            annotations={'sidecar.istio.io/inject': 'false'}\n",
    "        ),\n",
    "        spec=V1PodSpec(\n",
    "            containers=[container],\n",
    "            volumes=[efs_volume]\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bfe689b-0223-411c-96ab-cb51f08b8496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PyTorchJob custom resource manifest \n",
    "pytorchjob = V1PyTorchJob(\n",
    "    api_version=\"kubeflow.org/v1\",\n",
    "    kind=\"PyTorchJob\",\n",
    "    metadata=V1ObjectMeta(name=pytorch_distributed_jobname,namespace=user_namespace),\n",
    "    spec=V1PyTorchJobSpec(\n",
    "        run_policy=V1RunPolicy(clean_pod_policy=\"None\"),\n",
    "        pytorch_replica_specs={\"Master\": master,\n",
    "                               \"Worker\": worker}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2fc6df3-9a93-4b9b-a442-00d40ca5dfc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no existing job: pytorch-cnn-dist-job-2022-08-03-15-59-50-215. Please go ahead and create a new one\n"
     ]
    }
   ],
   "source": [
    "pytorchjob_client = PyTorchJobClient()\n",
    "\n",
    "try:\n",
    "  if(pytorchjob_client.get(pytorch_distributed_jobname, namespace=user_namespace)):\n",
    "    pytorchjob_client.delete(pytorch_distributed_jobname)\n",
    "    print(\"Existing job: %s deleted\"%(pytorch_distributed_jobname))\n",
    "except:\n",
    "  print(\"There is no existing job: %s. Please go ahead and create a new one\"%(pytorch_distributed_jobname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab60f8f9-262c-4ef2-8ccb-6314339b1e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates and Submits PyTorchJob custom resource file to Kubernetes\n",
    "pytorch_job_manifest=pytorchjob_client.create(pytorchjob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b444bd7c-e2cb-403f-80fa-6ff961db8d7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the submitted PyTorchJob custom resource file for reference \n",
    "\n",
    "# pytorch_job_manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d80fef-f59e-40e7-8848-ca0a58c5df8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function Definition: def save_master_worker_spec(pytorch_client: PyTorchJobClient, pytorch_jobname: str) -> str:\n",
    "#   Function also extracts master and worker spec that could be used for the creating the pipeline \n",
    "\n",
    "save_master_worker_spec(pytorchjob_client, pytorch_distributed_jobname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f15c6ac9-bb53-4d6e-bfcd-8249e218cf65",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The logs of Pod pytorch-cnn-dist-job-2022-08-03-15-59-50-215-master-0:\n",
      " Starting the script.\n",
      "Distributed training - True\n",
      "args.hosts - 3\n",
      "args.current_host - 0\n",
      "Initialized the distributed environment: 'gloo' backend on 3 nodes. \n",
      "data dir path - /efs-sc-claim/cifar10-dataset\n",
      "Get train data loader\n",
      "Get test data loader\n",
      "Processes 16667/50000 (33%) of train data\n",
      "Processes 10000/10000 (100%) of test data\n",
      "Train Epoch: 1 [3840/16667 (23%)] Loss: 2.309373\n",
      "Train Epoch: 1 [7680/16667 (46%)] Loss: 2.295434\n",
      "Train Epoch: 1 [11520/16667 (69%)] Loss: 2.280646\n",
      "Train Epoch: 1 [15360/16667 (92%)] Loss: 2.274296\n",
      "Test set: Average loss: -0.0276, Accuracy: 0.19\n",
      "\n",
      "Train Epoch: 2 [3840/16667 (23%)] Loss: 2.170421\n",
      "Train Epoch: 2 [7680/16667 (46%)] Loss: 2.028199\n",
      "Train Epoch: 2 [11520/16667 (69%)] Loss: 2.095306\n",
      "Train Epoch: 2 [15360/16667 (92%)] Loss: 1.852686\n",
      "Test set: Average loss: -0.6475, Accuracy: 0.29\n",
      "\n",
      "Train Epoch: 3 [3840/16667 (23%)] Loss: 1.831456\n",
      "Train Epoch: 3 [7680/16667 (46%)] Loss: 1.816702\n",
      "Train Epoch: 3 [11520/16667 (69%)] Loss: 2.031365\n",
      "Train Epoch: 3 [15360/16667 (92%)] Loss: 1.757967\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Pod condition to be Running\n",
      "Master and Worker Pods are Running now\n",
      "**** PyTorchJob status **** \n",
      "Running\n",
      "*************************** \n",
      "\n",
      "\n",
      "**** Pod names of the PyTorchJob **** \n",
      "{'pytorch-cnn-dist-job-2022-08-03-15-59-50-215-worker-0', 'pytorch-cnn-dist-job-2022-08-03-15-59-50-215-master-0', 'pytorch-cnn-dist-job-2022-08-03-15-59-50-215-worker-1'}\n",
      "*************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Function Definition: read_logs(pyTorchClient: str, jobname: str, namespace: str, log_type: str) -> None:\n",
    "#    log_type: all, worker:all, master:all, worker:0, worker:1\n",
    "\n",
    "read_logs(pytorchjob_client, pytorch_distributed_jobname, user_namespace, \"master:0\")\n",
    "\n",
    "# Useful Commands to run on Kubernetes control plane or in notebook using !. Substitute your namespace and pod names\n",
    "#  !kubectl get pods -n <aws-hybrid-training-ns>  \n",
    "#  !kubectl logs <pod-name> -n <aws-hybrid-training-ns> -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb0fc176-824d-482b-8fe2-224c100239d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the job succeeded\n",
    "\n",
    "pytorchjob_client.is_job_succeeded(pytorch_distributed_jobname, user_namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff159d90-1e44-4741-b5c3-42d24bbd1b8c",
   "metadata": {},
   "source": [
    "# CleanUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3be505a-e105-44c0-830e-e5ebe9ff6fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all previously submitted PyTorchJobs through this command. You can run in notebook as well on kubernetes cli \n",
    "\n",
    "#!kubectl get pytorchjob --no-headers=true -A | awk '/pytorch-cnn-dist/{print $2}' | xargs  kubectl delete pytorchjob  -n aws-hybrid-training-ns              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd22e62-7e95-4ecb-a307-59c7f7111fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
