{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb55227b-b029-4d95-9d80-8cf45fbf8637",
   "metadata": {},
   "source": [
    "## Notebook 2 - Operatioanlize Machine Learning Workflow using Hybrid ML Pipelines\n",
    "\n",
    "#### This notebook creates hybrid Kubeflow Pipelines that runs distributed training using either. PyTorch Training Operators on Kubernetes or Amazon SageMaker service based on conditional statements. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72ac6cc5-734c-4f5a-8218-70ca9b0b8f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries \n",
    "\n",
    "import kfp\n",
    "from kfp import components\n",
    "from kubeflow.training.utils import utils\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "import yaml\n",
    "import json\n",
    "from kubeflow.training import PyTorchJobClient\n",
    "import time\n",
    "import boto3\n",
    "import kfp.components as comp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64f0c848-49e1-4a7b-a927-89c7799ce6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize global variables \n",
    "\n",
    "user_namespace = utils.get_default_target_namespace()\n",
    "\n",
    "efs_mount_point='efs-sc-claim'\n",
    "\n",
    "aws_dlc_sagemaker_train_image='763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:1.8.0-gpu-py3'\n",
    "\n",
    "aws_dlc_sagemaker_inference_image='763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:1.6.0-gpu-py3'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5851c9a4-599a-4f47-b7fb-e25e21f9b235",
   "metadata": {},
   "source": [
    "# Create SageMaker session and default bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "951b091d-af2a-4d58-a507-956e7d639b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "# Import SageMaker specific libraries\n",
    "import sagemaker\n",
    "import boto3\n",
    "import random, string\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm= sess.client('sagemaker',region_name='us-west-2')\n",
    "\n",
    "# Use role as shared in WorkShop Steps \n",
    "role = 'arn:aws:iam::913278749917:role/sagemakerrole' \n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "dataset_folder = 'datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba24865e-cac1-42e1-b1cf-c91661828a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-west-2-913278749917'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SageMaker default bucket\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "job_folder      = 'jobs'\n",
    "dataset_folder  = 'datasets'\n",
    "local_dataset = 'cifar10'\n",
    "pytorchjob_name   = f'pytorch-dist-gpu-{time.strftime(\"%Y-%m-%d-%H-%M-%S-%j\", time.gmtime())}'\n",
    "\n",
    "bucket_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ed6178-c8dd-4cb9-8814-e685fd68f3b3",
   "metadata": {},
   "source": [
    "# Download the Cifar 10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "070449b9-30f7-4271-acb4-7d188c9b96fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "cifar10_dataset = torchvision.datasets.CIFAR10(efs_mount_point+'/cifar10-dataset', \n",
    "                                     train=True, \n",
    "                                     download=True)\n",
    "\n",
    "datasets = sagemaker_session.upload_data(path=efs_mount_point+'/cifar10-dataset', \n",
    "                                         key_prefix=f'{dataset_folder}/cifar10-dataset')\n",
    "\n",
    "#datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae2d7ce0-a518-4ce0-8c76-b43e9636d1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get kuberenetes pvc claim id for the provisioned efs from Kubeflow Volumes on the dashboard. eg. efs-sc-claim\n",
    "\n",
    "pvc_claim_id=!(kubectl get pvc --no-headers=true | awk '/efs-sc-claim/{print$3}' )\n",
    "\n",
    "#pvc_claim_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dcd687b-de41-4434-b447-40b371ba95e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "efs_fs_id=!kubectl describe pv $pvc_claim_id   | awk '/VolumeHandle/{print $2}' | cut -d':' -f1\n",
    "\n",
    "#efs_fs_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf96cabf-74e3-4bf6-9076-818e30149173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Subnet Id and Security Group for the EFS Mounted on your VPC Subnet\n",
    "\n",
    "client = boto3.client('efs')\n",
    "\n",
    "# Previous cell pre-populates File System Id. Please log in to AWS console and go to EFS service home page to verify it\n",
    "file_system_id=efs_fs_id[0]\n",
    "file_system_dir_path=\"/\"+pvc_claim_id[0]+\"/cifar10-dataset\"\n",
    "\n",
    "efs_mount_target_resp = client.describe_mount_targets(\n",
    "    MaxItems=123,\n",
    "    FileSystemId=file_system_id\n",
    ")\n",
    "\n",
    "subnet_id = efs_mount_target_resp['MountTargets'][0]['SubnetId']\n",
    "\n",
    "efs_mount_target_sg = client.describe_mount_target_security_groups(\n",
    "    MountTargetId=efs_mount_target_resp['MountTargets'][0]['MountTargetId']\n",
    ")\n",
    "\n",
    "security_group_id=efs_mount_target_sg['SecurityGroups'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92b2de15-d8ac-483d-99af-38d14cb739b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads SageMaker components for Kubeflow pipeline from the URL\n",
    "\n",
    "sagemaker_hpo_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2/components/aws/sagemaker/hyperparameter_tuning/component.yaml')\n",
    "sagemaker_train_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2/components/aws/sagemaker/train/component.yaml')\n",
    "sagemaker_model_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2/components/aws/sagemaker/model/component.yaml')\n",
    "sagemaker_deploy_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2/components/aws/sagemaker/deploy/component.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e23b5900-0e76-4ab7-8f31-20565764c0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./mnist.py\n",
      "./cifar10-distributed-gpu-final-Copy1.py\n",
      "./cifar10-distributed-gpu-final.py\n",
      "./model.pth\n",
      "\n",
      "Uploaded to S3 location:\n",
      "s3://sagemaker-us-west-2-913278749917/training-scripts/sourcedir.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Upload the training script to the S3 bucket to be accessed by SageMaker training job\n",
    "\n",
    "!tar cvfz sourcedir.tar.gz --exclude='./cifar10-dataset' --exclude='./*ipynb*' -C $efs_mount_point .\n",
    "source_s3 = sagemaker_session.upload_data(path='sourcedir.tar.gz', key_prefix='training-scripts')\n",
    "print('\\nUploaded to S3 location:')\n",
    "print(source_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "348c5c39-41bf-47ad-99f2-4feca9603113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read PyTorch Operator master and worker from the YAML file\n",
    "\n",
    "with open(\"pipeline_yaml_specifications/pipeline_master_spec.yml\", 'r') as master_stream:\n",
    "    master_spec_loaded = yaml.safe_load(master_stream)\n",
    "    \n",
    "with open(\"pipeline_yaml_specifications/pipeline_worker_spec.yml\", 'r') as worker_stream:\n",
    "    worker_spec_loaded = yaml.safe_load(worker_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93374827-f942-4fdf-92e3-dc2add89d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads PyTorch Training Operator component from the File\n",
    "\n",
    "pytorch_job_op = components.load_component_from_file('pipeline_components/pytorch_component.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb797562-e78a-4e69-b047-1c0164034915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check if the runtime passed to a pipeline is sagemaker or kubernetes\n",
    "def check_the_condition(training_runtime: str=\"sagemaker\") -> str:\n",
    "    if training_runtime.lower() == \"sagemaker\":\n",
    "        return \"sagemaker\"\n",
    "    elif training_runtime.lower() == \"kubernetes\":\n",
    "        return \"kubernetes\"\n",
    "        \n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea56d349-d581-47a5-8afe-ad048d217a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Python function-based components for Kubeflow Pipeline\n",
    "\n",
    "check_condition_op = comp.func_to_container_op(check_the_condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe1652e-b76b-462a-aebc-67ea4a6e6860",
   "metadata": {},
   "source": [
    "# Create hybrid Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "040c1c60-3d2e-460d-b222-a0ec2eb82d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create job name for tracking kuberenets PyTorchJob custom resource or SageMaker training job\n",
    "pytorch_distributed_jobname=f'pytorch-cnn-dist-job-{time.strftime(\"%Y-%m-%d-%H-%M-%S-%j\", time.gmtime())}'\n",
    "\n",
    "# Create Hybrid Pipeline using Kubeflow PyTorch Training Operators and Amazon SageMaker Service\n",
    "@dsl.pipeline(name=\"PyTorch Training pipeline\", description=\"Sample training job test\")\n",
    "def pytorch_cnn_pipeline(region='us-west-2',\n",
    "                           training_input_mode='File',\n",
    "                           namespace=user_namespace,\n",
    "                           train_image=aws_dlc_sagemaker_train_image,\n",
    "                           serving_image=aws_dlc_sagemaker_inference_image,\n",
    "                           volume_size='54',\n",
    "                           max_run_time='86400',\n",
    "                           learning_rate='0.01',\n",
    "                           pytorch_backend='gloo',\n",
    "                           training_job_name=pytorch_distributed_jobname, \n",
    "                           instance_type='ml.g4dn.12xlarge',\n",
    "                           network_isolation='False',\n",
    "                           traffic_encryption='False',\n",
    "                           spot_instance='False',\n",
    "                           training_runtime='kubernetes', # Define either sagemaker or kubernetes\n",
    "                           channels='[ \\\n",
    "                            { \\\n",
    "                                \"ChannelName\": \"train\", \\\n",
    "                                \"DataSource\": { \\\n",
    "                                    \"FileSystemDataSource\": { \\\n",
    "                                        \"FileSystemId\": \"'+file_system_id+'\", \\\n",
    "                                        \"FileSystemType\": \"EFS\", \\\n",
    "                                        \"FileSystemAccessMode\": \"ro\", \\\n",
    "                                        \"DirectoryPath\": \"'+file_system_dir_path+'\" \\\n",
    "                                    } \\\n",
    "                                }, \\\n",
    "                                \"CompressionType\": \"None\", \\\n",
    "                                \"RecordWrapperType\": \"None\" \\\n",
    "                            } \\\n",
    "                         ]'\n",
    "                        ):\n",
    "    \n",
    "    # Step to evaluate the condition. You can enter any logic here. For demonstration we are checking if GPU is needed for training \n",
    "    condition_result = check_condition_op(training_runtime)\n",
    "    \n",
    "    # Step to run training on Kuberentes using PyTorch Training Operators. This will be executed if gpus are not needed\n",
    "    with dsl.Condition(condition_result.output == 'kubernetes', name=\"PyTorch_Comp\"):\n",
    "        train_task = pytorch_job_op(\n",
    "            name=training_job_name, \n",
    "            namespace=user_namespace, \n",
    "            master_spec=json.dumps(master_spec_loaded), # Please refer file at pipeline_yaml_specifications/pipeline_master_spec.yml\n",
    "            worker_spec=json.dumps(worker_spec_loaded), # Please refer file at pipeline_yaml_specifications/pipeline_worker_spec.yml\n",
    "            delete_after_done=False\n",
    "        ).after(condition_result)\n",
    "    \n",
    "    # Step to run training on SageMaker using SageMaker Components for Pipeline. This will be executed if gpus are needed \n",
    "    with dsl.Condition(condition_result.output == 'sagemaker', name=\"SageMaker_Comp\"):\n",
    "        training = sagemaker_train_op(\n",
    "            region=region,\n",
    "            image=train_image,\n",
    "            job_name=training_job_name,\n",
    "            training_input_mode=training_input_mode,\n",
    "            hyperparameters='{ \\\n",
    "                \"backend\": \"'+str(pytorch_backend)+'\", \\\n",
    "                \"batch-size\": \"64\", \\\n",
    "                \"epochs\": \"3\", \\\n",
    "                \"lr\": \"'+str(learning_rate)+'\", \\\n",
    "                \"model-type\": \"custom\", \\\n",
    "                \"sagemaker_container_log_level\": \"20\", \\\n",
    "                \"sagemaker_program\": \"cifar10-distributed-gpu-final.py\", \\\n",
    "                \"sagemaker_region\": \"us-west-2\", \\\n",
    "                \"sagemaker_submit_directory\": \"'+source_s3+'\" \\\n",
    "            }',\n",
    "            channels=channels,\n",
    "            instance_type=instance_type,\n",
    "            instance_count=1,\n",
    "            volume_size=volume_size,\n",
    "            max_run_time=max_run_time,\n",
    "            model_artifact_path=f's3://{bucket_name}/jobs',\n",
    "            network_isolation=network_isolation,\n",
    "            traffic_encryption=traffic_encryption,\n",
    "            role=role,\n",
    "            vpc_subnets=subnet_id,\n",
    "            vpc_security_group_ids=security_group_id\n",
    "        ).after(condition_result)\n",
    "        \n",
    "    #Disable pipeline cache \n",
    "    train_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba18ad2-70d1-4192-8303-afc687c53fef",
   "metadata": {},
   "source": [
    "# Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed0af20e-13f5-460a-abb1-1326705def7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(pytorch_cnn_pipeline, \"pytorch_cnn_pipeline_new.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05209cb-7282-44ec-9776-1566860e2e40",
   "metadata": {},
   "source": [
    "# Execute the Pipeline using Kubeflow Pipeline Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39f151f5-9e77-43de-95db-87ddff00b526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/a3da6af6-9820-45e0-95a8-74150e87fcc0\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/b0bdc3ce-cf4c-4cea-8780-93405bfb7f05\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = kfp.Client()\n",
    "\n",
    "experiment = client.create_experiment(name=\"kubeflow\")\n",
    "\n",
    "my_run = client.run_pipeline(experiment.id, \"pytorch_cnn_pipeline\", \"pytorch_cnn_pipeline_new.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b7defb-a90e-4d11-a86d-b9a05074f72b",
   "metadata": {},
   "source": [
    "# CleanUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0bc119ac-860e-42d2-83c0-f47bb391b115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"pytorch-training-pipeline-hrz8x-4103682115\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete all previously submitted pipelines through this command. You can run in notebook as well on kubernetes cli \n",
    "\n",
    "!kubectl get pods --no-headers=true  | awk '/pytorch-training-pipeline/{print $1}' | xargs  kubectl delete pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f2bb5b2b-adc5-4336-9807-588ca09c85ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No resources found\n",
      "error: resource(s) were provided, but no name was specified\n"
     ]
    }
   ],
   "source": [
    "# Delete all previously submitted PyTorchJobs through this command. You can run in notebook as well on kubernetes cli \n",
    "\n",
    "!kubectl get pytorchjob --no-headers=true -A | awk '/pytorch-cnn-dist/{print $2}' | xargs  kubectl delete pytorchjob  -n aws-hybrid-training-ns              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80709b6b-b8c8-4338-a56f-ee3f8f27838c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
